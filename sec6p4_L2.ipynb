{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook we run the experiments of Section 6.4 without the pre-Cheeger regularisation; i.e., considering the problem over the $L^2$-space.\n",
    "\n",
    "First of all, we have to import the required libraries as well as the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from CF_NeuralNetwork import FFNN\n",
    "from CF_NeuralNetwork_PC import FFNNPC\n",
    "\n",
    "#Load the training set and normalize it\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train=X_train/X_train.sum(axis=(1,2))[:,None,None]\n",
    "X_train=np.reshape(X_train,(np.size(X_train,0),28*28))\n",
    "X_test=X_test/X_test.sum(axis=(1,2))[:,None,None]\n",
    "X_test=np.reshape(X_test,(np.size(X_test,0),28*28))\n",
    "D_train=np.loadtxt('MNIST_D_train', delimiter=',')\n",
    "D_test=np.loadtxt('MNIST_D_test', delimiter=',')\n",
    "Y_train=D_train\n",
    "Y_test=D_test\n",
    "\n",
    "x_Train_t=Variable(torch.from_numpy(X_train).float(), requires_grad=True)\n",
    "y_Train_t=Variable(torch.from_numpy(Y_train).float(), requires_grad=False)\n",
    "x_Test_t=Variable(torch.from_numpy(X_test).float(), requires_grad=False)\n",
    "y_Test_t=Variable(torch.from_numpy(Y_test).float(), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first define the $L^2$-norm and inner product, as well as the loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2norm(inp,model):\n",
    "    fs=0.0\n",
    "    pred=model(inp)\n",
    "    predt=torch.transpose(pred,0,1)\n",
    "    ts=pred.size(dim=0)\n",
    "    fs=1/ts*torch.mm(predt,pred)\n",
    "    res=torch.sqrt(fs)\n",
    "    return res.reshape(-1)\n",
    "\n",
    "def EulerLagrangeProduct(x_inp,y_inp,model_f,model_G):\n",
    "    S1=0\n",
    "    ts=x_inp.size(dim=0)\n",
    "    f_pred=model_f(x_inp)\n",
    "    y_inp=y_inp.reshape(ts,1)\n",
    "    fdiff=f_pred-y_inp\n",
    "    G_pred=model_G(x_inp)\n",
    "    G_pred_t=torch.transpose(G_pred,0,1)\n",
    "    S1=1/ts*torch.mm(G_pred_t,fdiff)\n",
    "    val=S1\n",
    "    S2S=L2norm(x_inp,model_G)\n",
    "    val=val/S2S\n",
    "    return val\n",
    "\n",
    "def LossFunction_f(x_inp,y_inp,model_f,model_G):\n",
    "    prod=EulerLagrangeProduct(x_inp,y_inp,model_f,model_G)\n",
    "    loss=torch.abs(prod)\n",
    "    return loss\n",
    "\n",
    "def LossFunction_G(x_inp,y_inp,model_f,model_G):\n",
    "    prod=EulerLagrangeProduct(x_inp,y_inp,model_f,model_G)\n",
    "    loss=-1.0*torch.abs(prod)\n",
    "    return loss\n",
    "\n",
    "def my_rel_loss(output,target):\n",
    "    loss=torch.abs(output-target)\n",
    "    loss=loss/target\n",
    "    loss=torch.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the choices $N_\\Xi =1$ and $N_\\Theta=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the neural network based on npot potentials\n",
    "nd=x_Train_t.size(dim=1)\n",
    "npot=2**7\n",
    "model_f=FFNN(nd,npot,non_trainable=False)\n",
    "model_G=FFNN(nd,npot,non_trainable=False)\n",
    "optimizer_f = torch.optim.Adam(model_f.parameters())\n",
    "optimizer_G = torch.optim.Adam(model_G.parameters())\n",
    "\n",
    "epochs_f=2\n",
    "epochs_G=1\n",
    "loops=300\n",
    "batch_size = 64\n",
    "\n",
    "epoch_MRE=[]\n",
    "epoch_MRE_Test=[]\n",
    "epoch_list=[]\n",
    "\n",
    "for loopc in range(loops):\n",
    "    for epochs_G_c in range(epochs_G):\n",
    "        permutation = torch.randperm(x_Train_t.size()[0])\n",
    "        for i in range(0,x_Train_t.size()[0], batch_size):\n",
    "            optimizer_G.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "            loss=LossFunction_G(batch_x,batch_y,model_f,model_G)\n",
    "            loss.backward()\n",
    "            optimizer_G.step()\n",
    "    for epochs_f_c in range(epochs_f):\n",
    "        permutation = torch.randperm(x_Train_t.size()[0])\n",
    "        acc_MRE=0\n",
    "        acc_Loss=0\n",
    "        counter=0\n",
    "        for i in range(0,x_Train_t.size()[0], batch_size):\n",
    "            optimizer_f.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "            loss=LossFunction_f(batch_x,batch_y,model_f,model_G)\n",
    "            loss.backward()\n",
    "            optimizer_f.step()\n",
    "            y_pred=model_f(batch_x)\n",
    "            y_pred=y_pred.reshape(-1)\n",
    "            acc_MRE+=my_rel_loss(y_pred,batch_y)\n",
    "            acc_Loss+=loss\n",
    "            counter+=1\n",
    "    y_pred_test=model_f(x_Test_t)\n",
    "    y_pred_test=y_pred_test.reshape(-1)\n",
    "    MRE_Test=my_rel_loss(y_pred_test,y_Test_t)\n",
    "    epoch_MRE_Test.append(MRE_Test.detach().numpy())\n",
    "    MRE=acc_MRE/counter\n",
    "    epoch_MRE.append(MRE.detach().numpy())\n",
    "    epoch_list.append(loopc+1)\n",
    "\n",
    "epoch_MRE=np.array(epoch_MRE)\n",
    "epoch_MRE_Test=np.array(epoch_MRE_Test)\n",
    "counter=np.array(epoch_list)\n",
    "\n",
    "#Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0.01, 0.5])\n",
    "plt.xlabel('number of epochs',fontsize=16)\n",
    "plt.ylabel('mean relative error',fontsize=16)\n",
    "plot_1a, =ax.semilogy(counter,epoch_MRE,color='blue',label='training set')\n",
    "plot_1b, =ax.semilogy(counter,epoch_MRE_Test,'--',color='green',label='test set')\n",
    "plt.axhline(y = 0.1594, color = 'r', linestyle = ':')\n",
    "ax.legend(handles=[plot_1a,plot_1b],loc='upper right',fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.grid()\n",
    "plt.grid(which='minor',alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set $N_\\Xi =2$ and $N_\\Theta=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the neural network based on npot potentials\n",
    "nd=x_Train_t.size(dim=1)\n",
    "npot=2**7\n",
    "model_f=FFNN(nd,npot,non_trainable=False)\n",
    "model_G=FFNN(nd,npot,non_trainable=False)\n",
    "optimizer_f = torch.optim.Adam(model_f.parameters())\n",
    "optimizer_G = torch.optim.Adam(model_G.parameters())\n",
    "\n",
    "epochs_f=1\n",
    "epochs_G=2\n",
    "loops=300\n",
    "batch_size = 64\n",
    "\n",
    "epoch_MRE=[]\n",
    "epoch_MRE_Test=[]\n",
    "epoch_list=[]\n",
    "\n",
    "for loopc in range(loops):\n",
    "    for epochs_G_c in range(epochs_G):\n",
    "        permutation = torch.randperm(x_Train_t.size()[0])\n",
    "        for i in range(0,x_Train_t.size()[0], batch_size):\n",
    "            optimizer_G.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "            loss=LossFunction_G(batch_x,batch_y,model_f,model_G)\n",
    "            loss.backward()\n",
    "            optimizer_G.step()\n",
    "    for epochs_f_c in range(epochs_f):\n",
    "        permutation = torch.randperm(x_Train_t.size()[0])\n",
    "        acc_MRE=0\n",
    "        acc_Loss=0\n",
    "        counter=0\n",
    "        for i in range(0,x_Train_t.size()[0], batch_size):\n",
    "            optimizer_f.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "            loss=LossFunction_f(batch_x,batch_y,model_f,model_G)\n",
    "            loss.backward()\n",
    "            optimizer_f.step()\n",
    "            y_pred=model_f(batch_x)\n",
    "            y_pred=y_pred.reshape(-1)\n",
    "            acc_MRE+=my_rel_loss(y_pred,batch_y)\n",
    "            acc_Loss+=loss\n",
    "            counter+=1\n",
    "    y_pred_test=model_f(x_Test_t)\n",
    "    y_pred_test=y_pred_test.reshape(-1)\n",
    "    MRE_Test=my_rel_loss(y_pred_test,y_Test_t)\n",
    "    epoch_MRE_Test.append(MRE_Test.detach().numpy())\n",
    "    MRE=acc_MRE/counter\n",
    "    epoch_MRE.append(MRE.detach().numpy())\n",
    "    epoch_list.append(loopc+1)\n",
    "\n",
    "epoch_MRE=np.array(epoch_MRE)\n",
    "epoch_MRE_Test=np.array(epoch_MRE_Test)\n",
    "counter=np.array(epoch_list)\n",
    "\n",
    "#Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0.01, 0.5])\n",
    "plt.xlabel('number of epochs',fontsize=16)\n",
    "plt.ylabel('mean relative error',fontsize=16)\n",
    "plot_1a, =ax.semilogy(counter,epoch_MRE,color='blue',label='training set')\n",
    "plot_1b, =ax.semilogy(counter,epoch_MRE_Test,'--',color='green',label='test set')\n",
    "plt.axhline(y = 0.1594, color = 'r', linestyle = ':')\n",
    "ax.legend(handles=[plot_1a,plot_1b],loc='upper right',fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.grid()\n",
    "plt.grid(which='minor',alpha=0.2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
