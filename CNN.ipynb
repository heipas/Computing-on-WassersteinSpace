{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test a convolutional neural network (CNN) for the approximation of the Wasserstein distance. The architecture is borrowed, up to some modification, from https://www.kaggle.com/code/shadabhussain/cifar-10-cnn-using-pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from CF_NeuralNetwork import FFNN\n",
    "from CF_NeuralNetwork_PC import FFNNPC\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Extract raw image data and labels\n",
    "X_train = trainset.data\n",
    "X_test = testset.data\n",
    "\n",
    "# Normalize each image and scale by 32*32\n",
    "X_train_normalized = X_train / X_train.sum(axis=(1, 2, 3))[:, None, None, None] * 32 * 32\n",
    "X_test_normalized = X_test / X_test.sum(axis=(1, 2, 3))[:, None, None, None] * 32 * 32\n",
    "\n",
    "# Remove the reference image at index 34\n",
    "X_reference = X_train_normalized[34]\n",
    "X_train_normalized = np.delete(X_train_normalized, 34, axis=0)\n",
    "\n",
    "# Convert normalized data back to tensors\n",
    "x_Train_t = torch.Tensor(X_train_normalized).permute(0, 3, 1, 2)  # Convert to (N, C, H, W) shape\n",
    "\n",
    "x_Test_t = torch.Tensor(X_test_normalized).permute(0, 3, 1, 2)  # Convert to (N, C, H, W) shape\n",
    "\n",
    "Y_train=np.loadtxt('CIFAR_D_train', delimiter=',')\n",
    "Y_test=np.loadtxt('CIFAR_D_test', delimiter=',')\n",
    "Y_train=np.delete(Y_train,34,0)\n",
    "\n",
    "\n",
    "# Transform data to torch variables\n",
    "# x_Train_t=Variable(torch.from_numpy(X_train).float(), requires_grad=True)\n",
    "y_Train_t=Variable(torch.from_numpy(Y_train).float(), requires_grad=False)\n",
    "#x_Test_t=Variable(torch.from_numpy(X_test).float(), requires_grad=False)\n",
    "y_Test_t=Variable(torch.from_numpy(Y_test).float(), requires_grad=False)\n",
    "\n",
    "def my_rel_loss(output,target):\n",
    "    loss=torch.abs(output-target)\n",
    "    loss=torch.div(loss,target)\n",
    "    loss=torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def my_abs_loss(output,target):\n",
    "    loss=torch.abs(output-target)\n",
    "    loss=torch.mean(loss)\n",
    "    return loss.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a CNN with 6,896,321 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "    \n",
    "model = Cifar10Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the neural network\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs = 300\n",
    "batch_size = 64\n",
    "\n",
    "# Training Loss\n",
    "epoch_Loss=[]\n",
    "# List for the epochs\n",
    "epoch_list=[]\n",
    "# Test Loss\n",
    "epoch_Loss_Val=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    acc_loss = 0\n",
    "    counter = 0\n",
    "    \n",
    "    # Shuffle the dataset manually\n",
    "    permutation = torch.randperm(x_Train_t.size()[0])\n",
    "    \n",
    "    for i in range(0, x_Train_t.size(0), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        # Batch selection\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "        # Forward pass\n",
    "        y_pred = model(batch_x)\n",
    "        y_pred = y_pred.reshape(-1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = my_rel_loss(y_pred, batch_y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc_loss += loss\n",
    "        counter += 1\n",
    "\n",
    "    # Calculate average loss\n",
    "    loss_app=acc_loss/counter\n",
    "    print(loss_app)\n",
    "    y_pred_val=model(x_Test_t)\n",
    "    y_pred_val=y_pred_val.reshape(-1)\n",
    "    loss_Val=my_rel_loss(y_pred_val,y_Test_t)\n",
    "    epoch_Loss_Val.append(loss_Val.detach().numpy())\n",
    "    epoch_Loss.append(loss_app.detach().numpy())\n",
    "    epoch_list.append(epoch+1)\n",
    "\n",
    "\n",
    "# Create the numpy arrays for the plots\n",
    "counter=np.array(epoch_list)\n",
    "mse_loss=np.array(epoch_Loss)\n",
    "mse_loss_val=np.array(epoch_Loss_Val)\n",
    "\n",
    "# Generate the plots\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0.001, 1])\n",
    "plt.xlabel('number of epochs',fontsize=16)\n",
    "plt.ylabel('mean relative error',fontsize=16)\n",
    "plot_1a, =ax.semilogy(counter,mse_loss,color='blue',label='training set')\n",
    "plot_1b, =ax.semilogy(counter,mse_loss_val,'--',color='green',label='test set')\n",
    "plt.axhline(y = 0.037183734101399565, color = 'r', linestyle = ':')\n",
    "ax.legend(handles=[plot_1a,plot_1b],loc='upper right',fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.grid()\n",
    "plt.grid(which='minor',alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
