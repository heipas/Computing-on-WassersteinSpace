{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c8a2728",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook we run the experiments of Section 6.3.\n",
    "\n",
    "First of all, we have to import the required libraries as well as the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bcb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from CF_NeuralNetwork import FFNN\n",
    "from CF_NeuralNetwork_PC import FFNNPC\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transforms.ToTensor(),\n",
    "                                        download=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor(),\n",
    "                                       download=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, \n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "X_train=trainset.data\n",
    "Y_train=trainset.targets\n",
    "X_test=testset.data\n",
    "Y_test=testset.targets\n",
    "Y_train_alt=np.array(Y_train)\n",
    "X_train=X_train/X_train.sum(axis=(1,2,3))[:,None,None,None]*32*32\n",
    "X_train=np.reshape(X_train,(np.size(X_train,0),32*32*3))\n",
    "X_reference=X_train[34]\n",
    "X_reference=np.reshape(X_reference,32*32*3)\n",
    "X_test=X_test/X_test.sum(axis=(1,2,3))[:,None,None,None]*32*32\n",
    "X_test=np.reshape(X_test,(np.size(X_test,0),32*32*3))\n",
    "Y_train=np.loadtxt('CIFAR_D_train', delimiter=',')\n",
    "Y_test=np.loadtxt('CIFAR_D_test', delimiter=',')\n",
    "X_train=np.delete(X_train,34,0)\n",
    "Y_train=np.delete(Y_train,34,0)\n",
    "\n",
    "\n",
    "# Transform data to torch variables\n",
    "x_Train_t=Variable(torch.from_numpy(X_train).float(), requires_grad=True)\n",
    "y_Train_t=Variable(torch.from_numpy(Y_train).float(), requires_grad=False)\n",
    "x_Test_t=Variable(torch.from_numpy(X_test).float(), requires_grad=False)\n",
    "y_Test_t=Variable(torch.from_numpy(Y_test).float(), requires_grad=False)\n",
    "\n",
    "def my_rel_loss(output,target):\n",
    "    loss=torch.abs(output-target)\n",
    "    loss=torch.div(loss,target)\n",
    "    loss=torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "def my_abs_loss(output,target):\n",
    "    loss=torch.abs(output-target)\n",
    "    loss=torch.mean(loss)\n",
    "    return loss.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff78413",
   "metadata": {},
   "source": [
    "Experiment 6.6: We consider the same neural network architecture as in Experiment 6.5. However, in contrast to the experiments before, we allow all the neural network parameters to be trainable, including the ones from the part corresponding\n",
    "to the representation of the maxima function. Nonetheless, we initialise the second part of the architecture to represent the maxima function. We start by randomly initialising the first hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383751cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the neural network\n",
    "dimension_space=x_Train_t.size(dim=1)\n",
    "number_potentials=2**12\n",
    "model=FFNN(dimension_space,number_potentials,False)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs=300\n",
    "batch_size = 64\n",
    "\n",
    "# Training Loss\n",
    "epoch_Loss=[]\n",
    "# List for the epochs\n",
    "epoch_list=[]\n",
    "# Test Loss\n",
    "epoch_Loss_Val=[]\n",
    "\n",
    "for epochc in range(epochs):\n",
    "    permutation = torch.randperm(x_Train_t.size()[0])\n",
    "    acc_loss=0\n",
    "    counter=0\n",
    "    for i in range(0,x_Train_t.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "        y_pred=model(batch_x)\n",
    "        y_pred=y_pred.reshape(-1)\n",
    "        loss=my_rel_loss(y_pred,batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc_loss += loss\n",
    "        counter += 1\n",
    "    loss_app=acc_loss/counter\n",
    "    y_pred_val=model(x_Test_t)\n",
    "    y_pred_val=y_pred_val.reshape(-1)\n",
    "    loss_Val=my_rel_loss(y_pred_val,y_Test_t)\n",
    "    epoch_Loss_Val.append(loss_Val.detach().numpy())\n",
    "    epoch_Loss.append(loss_app.detach().numpy())\n",
    "    epoch_list.append(epochc+1)\n",
    "\n",
    "# Create the numpy arrays for the plots\n",
    "counter=np.array(epoch_list)\n",
    "mse_loss=np.array(epoch_Loss)\n",
    "mse_loss_val=np.array(epoch_Loss_Val)\n",
    "\n",
    "# Generate the plots\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0.001, 1])\n",
    "plt.xlabel('number of epochs',fontsize=16)\n",
    "plt.ylabel('mean relative error',fontsize=16)\n",
    "plot_1a, =ax.semilogy(counter,mse_loss,color='blue',label='training set')\n",
    "plot_1b, =ax.semilogy(counter,mse_loss_val,'--',color='green',label='test set')\n",
    "plt.axhline(y = 0.037183734101399565, color = 'r', linestyle = ':')\n",
    "ax.legend(handles=[plot_1a,plot_1b],loc='upper right',fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.grid()\n",
    "plt.grid(which='minor',alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815befae",
   "metadata": {},
   "source": [
    "Next, we use the precomputed Kantorovich potentials to initialise the first hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e973fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "B0f=np.loadtxt('CIFAR_P_f.gz', delimiter=',')\n",
    "B0g=np.loadtxt('CIFAR_P_g.gz', delimiter=',')\n",
    "bvectot=np.matmul(B0g,X_reference)\n",
    "idx=np.arange(B0f.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "BS=B0f[idx,:]\n",
    "bvecS=bvectot[idx]\n",
    "dimension_space=x_Train_t.size(dim=1)\n",
    "number_potentials=2**12\n",
    "Amat=BS[:number_potentials,:]\n",
    "bvec=bvecS[0:number_potentials]\n",
    "\n",
    "model=FFNNPC(Amat,bvec,dimension_space,number_potentials,False)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs = 300\n",
    "batch_size = 64\n",
    "# Training Loss\n",
    "epoch_Loss=[]\n",
    "# List for the epochs\n",
    "epoch_list=[]\n",
    "# Test Loss\n",
    "epoch_Loss_Val=[]\n",
    "\n",
    "# Initial erros; i.e., based on the pre-computed potentials, but without any training\n",
    "y_pred_train=model(x_Train_t)\n",
    "y_pred_train=y_pred_train.reshape(-1)\n",
    "loss_train=my_rel_loss(y_pred_train,y_Train_t)\n",
    "print(loss_train)\n",
    "y_pred_test=model(x_Test_t)\n",
    "y_pred_test=y_pred_test.reshape(-1)\n",
    "loss_test=my_rel_loss(y_pred_test,y_Test_t)\n",
    "epoch_Loss_Val.append(loss_test.detach().numpy())\n",
    "epoch_Loss.append(loss_train.detach().numpy())\n",
    "epoch_list.append(0)\n",
    "\n",
    "for epochc in range(epochs):\n",
    "    permutation = torch.randperm(x_Train_t.size()[0])\n",
    "    acc_loss=0\n",
    "    counter=0\n",
    "    for i in range(0,x_Train_t.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "        y_pred=model(batch_x)\n",
    "        y_pred=y_pred.reshape(-1)\n",
    "        loss=my_rel_loss(y_pred,batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc_loss += loss\n",
    "        counter += 1\n",
    "    loss_app=acc_loss/counter\n",
    "    y_pred_val=model(x_Test_t)\n",
    "    y_pred_val=y_pred_val.reshape(-1)\n",
    "    loss_Val=my_rel_loss(y_pred_val,y_Test_t)\n",
    "    epoch_Loss_Val.append(loss_Val.detach().numpy())\n",
    "    epoch_Loss.append(loss_app.detach().numpy())\n",
    "    epoch_list.append(epochc+1)\n",
    "\n",
    "# Create numpy arrays for the plots\n",
    "counter=np.array(epoch_list)\n",
    "mse_loss=np.array(epoch_Loss)\n",
    "mse_loss_val=np.array(epoch_Loss_Val)\n",
    "\n",
    "\n",
    "# Generate the plots\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0.001, 1])\n",
    "plt.xlabel('number of epochs',fontsize=16)\n",
    "plt.ylabel('mean relative error',fontsize=16)\n",
    "plot_1a, =ax.semilogy(counter,mse_loss,color='blue',label='training set')\n",
    "plot_1b, =ax.semilogy(counter,mse_loss_val,'--',color='green',label='test set')\n",
    "plt.axhline(y = 0.037183734101399565, color = 'r', linestyle = ':')\n",
    "ax.legend(handles=[plot_1a,plot_1b],loc='upper right',fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.grid()\n",
    "plt.grid(which='minor',alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55105ae2",
   "metadata": {},
   "source": [
    "Next, we shall prepare the discretised $H^{1,2}$-norm, which will be employed in the Experiment 6.7 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae13fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nth_derivative(f, wrt, n):\n",
    "    for i in range(n):\n",
    "        grads = grad(f.sum(), wrt, create_graph=True, allow_unused=True)[0]\n",
    "    return grads\n",
    "\n",
    "def phigradx(model):\n",
    "    a=model.layers[0].weight.data\n",
    "    f=a.size(dim=0)\n",
    "    a=a.reshape(f,32,32,3)\n",
    "    b=torch.zeros(f,32,32,3)\n",
    "    b[:,0:-1,:,:]=a[:,1:,:,:]-a[:,0:-1,:,:]\n",
    "    b[:,-1,:,:]=a[:,-1,:,:]-a[:,-2,:,:]\n",
    "    c=b.reshape(f,32*32*3)\n",
    "    return c\n",
    "\n",
    "def phigrady(model):\n",
    "    a=model.layers[0].weight.data\n",
    "    f=a.size(dim=0)\n",
    "    a=a.reshape(f,32,32,3)\n",
    "    b=torch.zeros(f,32,32,3)\n",
    "    b[:,:,0:-1,:]=a[:,:,1:,:]-a[:,:,0:-1,:]\n",
    "    b[:,:,-1,:]=a[:,:,-1,:]-a[:,:,-2,:]\n",
    "    c=b.reshape(f,32*32*3)\n",
    "    return c\n",
    "\n",
    "def phigradRGB(model):\n",
    "    a=model.layers[0].weight.data\n",
    "    f=a.size(dim=0)\n",
    "    a=a.reshape(f,32,32,3)\n",
    "    b=torch.zeros(f,32,32,3)\n",
    "    b[:,:,:,0:-1]=a[:,:,:,1:]-a[:,:,:,0:-1]\n",
    "    b[:,:,:,-1]=a[:,:,:,-1]-a[:,:,:,-2]\n",
    "    c=b.reshape(f,32*32*3)\n",
    "    return c\n",
    "\n",
    "def H12norm(inp,model):\n",
    "    dfs=0.0\n",
    "    fs=0.0\n",
    "    pred=model(inp)\n",
    "    ts=pred.size(dim=0)\n",
    "    gpx=phigradx(model)\n",
    "    gpy=phigrady(model)\n",
    "    gpz=phigradRGB(model)\n",
    "    inp2=model.modelPhi(inp)\n",
    "    Psi=model.modelPsi(inp2)\n",
    "    dPsi=nth_derivative(Psi,wrt=inp2,n=1)\n",
    "    Dvecx=torch.mm(dPsi,gpx)\n",
    "    Dvecy=torch.mm(dPsi,gpy)\n",
    "    Dvecz=torch.mm(dPsi,gpz)\n",
    "    Dvecxsq=torch.square(Dvecx)\n",
    "    Dvecysq=torch.square(Dvecy)\n",
    "    Dveczsq=torch.square(Dvecz)\n",
    "    Dvecsq=Dvecxsq+Dvecysq+Dveczsq\n",
    "    dfsvec=torch.mul(Dvecsq,inp)\n",
    "    dfs=torch.sum(dfsvec)\n",
    "    dfs=1/ts*dfs\n",
    "    predt=torch.transpose(pred,0,1)\n",
    "    fs=1/ts*torch.mm(predt,pred)\n",
    "    res=torch.sqrt(dfs+fs)\n",
    "    return res.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac04ca",
   "metadata": {},
   "source": [
    "Experiment 6.7: We repeat the experiment from before, but add a regularisation by means of the discretised $H^{1,2}$-norm. First, the regularisation parameter is set to $\\lambda=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the neural network\n",
    "dimension_space=x_Train_t.size(dim=1)\n",
    "number_potentials=2**10\n",
    "model=FFNN(dimension_space,number_potentials,False)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs=300\n",
    "batch_size = 64\n",
    "lambdas = 0.1\n",
    "\n",
    "# Training Loss\n",
    "epoch_Loss=[]\n",
    "# List for the epochs\n",
    "epoch_list=[]\n",
    "# Test Loss\n",
    "epoch_Loss_Val=[]\n",
    "\n",
    "for epochc in range(epochs):\n",
    "    permutation = torch.randperm(x_Train_t.size()[0])\n",
    "    acc_loss=0\n",
    "    counter=0\n",
    "    for i in range(0,x_Train_t.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "        y_pred=model(batch_x)\n",
    "        y_pred=y_pred.reshape(-1)\n",
    "        loss=my_abs_loss(y_pred,batch_y)\n",
    "        h12n=H12norm(batch_x,model)\n",
    "        lossO=loss+lambdas*H12norm(batch_x,model)\n",
    "        lossO.backward()\n",
    "        optimizer.step()\n",
    "        acc_loss += my_rel_loss(y_pred,batch_y)\n",
    "        counter += 1\n",
    "    loss_app=acc_loss/counter\n",
    "    y_pred_val=model(x_Test_t)\n",
    "    y_pred_val=y_pred_val.reshape(-1)\n",
    "    loss_Val=my_rel_loss(y_pred_val,y_Test_t)\n",
    "    epoch_Loss_Val.append(loss_Val.detach().numpy())\n",
    "    epoch_Loss.append(loss_app.detach().numpy())\n",
    "    epoch_list.append(epochc+1)\n",
    "\n",
    "# Create the numpy arrays for the plots\n",
    "counter=np.array(epoch_list)\n",
    "mse_loss=np.array(epoch_Loss)\n",
    "mse_loss_val=np.array(epoch_Loss_Val)\n",
    "\n",
    "# Generate the plots\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0.001, 1])\n",
    "plt.xlabel('number of epochs',fontsize=16)\n",
    "plt.ylabel('mean relative error',fontsize=16)\n",
    "plot_1a, =ax.semilogy(counter,mse_loss,color='blue',label='training set')\n",
    "plot_1b, =ax.semilogy(counter,mse_loss_val,'--',color='green',label='test set')\n",
    "plt.axhline(y = 0.037183734101399565, color = 'r', linestyle = ':')\n",
    "ax.legend(handles=[plot_1a,plot_1b],loc='upper right',fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.grid()\n",
    "plt.grid(which='minor',alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631715f",
   "metadata": {},
   "source": [
    "Next, the regularisation parameter is set to be $\\lambda=0.001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b26dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the neural network\n",
    "dimension_space=x_Train_t.size(dim=1)\n",
    "number_potentials=2**10\n",
    "model=FFNN(dimension_space,number_potentials,False)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs=300\n",
    "batch_size = 64\n",
    "lambdas = 0.001\n",
    "\n",
    "# Training Loss\n",
    "epoch_Loss=[]\n",
    "# List for the epochs\n",
    "epoch_list=[]\n",
    "# Test Loss\n",
    "epoch_Loss_Val=[]\n",
    "\n",
    "for epochc in range(epochs):\n",
    "    permutation = torch.randperm(x_Train_t.size()[0])\n",
    "    acc_loss=0\n",
    "    counter=0\n",
    "    for i in range(0,x_Train_t.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "        y_pred=model(batch_x)\n",
    "        y_pred=y_pred.reshape(-1)\n",
    "        loss=my_abs_loss(y_pred,batch_y)\n",
    "        h12n=H12norm(batch_x,model)\n",
    "        lossO=loss+lambdas*H12norm(batch_x,model)\n",
    "        lossO.backward()\n",
    "        optimizer.step()\n",
    "        acc_loss += my_rel_loss(y_pred,batch_y)\n",
    "        counter += 1\n",
    "    loss_app=acc_loss/counter\n",
    "    y_pred_val=model(x_Test_t)\n",
    "    y_pred_val=y_pred_val.reshape(-1)\n",
    "    loss_Val=my_rel_loss(y_pred_val,y_Test_t)\n",
    "    epoch_Loss_Val.append(loss_Val.detach().numpy())\n",
    "    epoch_Loss.append(loss_app.detach().numpy())\n",
    "    epoch_list.append(epochc+1)\n",
    "\n",
    "# Create the numpy arrays for the plots\n",
    "counter=np.array(epoch_list)\n",
    "mse_loss=np.array(epoch_Loss)\n",
    "mse_loss_val=np.array(epoch_Loss_Val)\n",
    "\n",
    "# Generate the plots\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0.001, 1])\n",
    "plt.xlabel('number of epochs',fontsize=16)\n",
    "plt.ylabel('mean relative error',fontsize=16)\n",
    "plot_1a, =ax.semilogy(counter,mse_loss,color='blue',label='training set')\n",
    "plot_1b, =ax.semilogy(counter,mse_loss_val,'--',color='green',label='test set')\n",
    "plt.axhline(y = 0.037183734101399565, color = 'r', linestyle = ':')\n",
    "ax.legend(handles=[plot_1a,plot_1b],loc='upper right',fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.grid()\n",
    "plt.grid(which='minor',alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b4a72",
   "metadata": {},
   "source": [
    "In Experiment 6.8, we compare the performance of our neural network archictecture from Experiment 6.6 against a convolutional neural network (CNN). The architecture of the CNN is borrowed, up to some modification, from https://www.kaggle.com/code/shadabhussain/cifar-10-cnn-using-pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551f42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CF_CNN import Cifar10Model\n",
    "\n",
    "model = Cifar10Model()\n",
    "\n",
    "# Initialize the neural network\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs = 300\n",
    "batch_size = 64\n",
    "\n",
    "# Training Loss\n",
    "epoch_Loss=[]\n",
    "# List for the epochs\n",
    "epoch_list=[]\n",
    "# Test Loss\n",
    "epoch_Loss_Val=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    acc_loss = 0\n",
    "    counter = 0\n",
    "    \n",
    "    # Shuffle the dataset manually\n",
    "    permutation = torch.randperm(x_Train_t.size()[0])\n",
    "    \n",
    "    for i in range(0, x_Train_t.size(0), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        # Batch selection\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "        # Forward pass\n",
    "        y_pred = model(batch_x)\n",
    "        y_pred = y_pred.reshape(-1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = my_rel_loss(y_pred, batch_y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc_loss += loss\n",
    "        counter += 1\n",
    "\n",
    "    # Calculate average loss\n",
    "    loss_app=acc_loss/counter\n",
    "    print(loss_app)\n",
    "    y_pred_val=model(x_Test_t)\n",
    "    y_pred_val=y_pred_val.reshape(-1)\n",
    "    loss_Val=my_rel_loss(y_pred_val,y_Test_t)\n",
    "    epoch_Loss_Val.append(loss_Val.detach().numpy())\n",
    "    epoch_Loss.append(loss_app.detach().numpy())\n",
    "    epoch_list.append(epoch+1)\n",
    "\n",
    "\n",
    "# Create the numpy arrays for the plots\n",
    "counter=np.array(epoch_list)\n",
    "mse_loss=np.array(epoch_Loss)\n",
    "mse_loss_val=np.array(epoch_Loss_Val)\n",
    "\n",
    "# Generate the plots\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0.001, 1])\n",
    "plt.xlabel('number of epochs',fontsize=16)\n",
    "plt.ylabel('mean relative error',fontsize=16)\n",
    "plot_1a, =ax.semilogy(counter,mse_loss,color='blue',label='training set')\n",
    "plot_1b, =ax.semilogy(counter,mse_loss_val,'--',color='green',label='test set')\n",
    "plt.axhline(y = 0.037183734101399565, color = 'r', linestyle = ':')\n",
    "ax.legend(handles=[plot_1a,plot_1b],loc='upper right',fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.grid()\n",
    "plt.grid(which='minor',alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
