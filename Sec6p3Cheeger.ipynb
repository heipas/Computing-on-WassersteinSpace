{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook we run the experiments of Section 6.3 with the pre-Cheeger regularisation.\n",
    "\n",
    "First of all, we have to import the required libraries as well as the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from CF_NeuralNetwork import FFNN\n",
    "from CF_NeuralNetwork_PC import FFNNPC\n",
    "\n",
    "#Load the training set and normalize it\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train=X_train/X_train.sum(axis=(1,2))[:,None,None]\n",
    "X_train=np.reshape(X_train,(np.size(X_train,0),28*28))\n",
    "X_test=X_test/X_test.sum(axis=(1,2))[:,None,None]\n",
    "X_test=np.reshape(X_test,(np.size(X_test,0),28*28))\n",
    "D_train=np.loadtxt('MNIST_D_train', delimiter=',')\n",
    "D_test=np.loadtxt('MNIST_D_test', delimiter=',')\n",
    "Y_train=D_train\n",
    "Y_test=D_test\n",
    "\n",
    "x_Train_t=Variable(torch.from_numpy(X_train).float(), requires_grad=True)\n",
    "y_Train_t=Variable(torch.from_numpy(Y_train).float(), requires_grad=False)\n",
    "x_Test_t=Variable(torch.from_numpy(X_test).float(), requires_grad=False)\n",
    "y_Test_t=Variable(torch.from_numpy(Y_test).float(), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define the $H^{1,2}$-norm and the inner product for the Euler-Lagrange formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nth_derivative(f, wrt, n):\n",
    "    for i in range(n):\n",
    "        grads = grad(f.sum(), wrt, create_graph=True, allow_unused=True)[0]\n",
    "        f = grads\n",
    "    return grads\n",
    "\n",
    "def phigrady(model):\n",
    "    a=model.layers[0].weight.data\n",
    "    f=a.size(dim=0)\n",
    "    a=a.reshape(f,28,28)\n",
    "    b=torch.zeros(f,28,28)\n",
    "    b[:,0:-1,:]=a[:,1:,:]-a[:,0:-1,:]\n",
    "    b[:,-1,:]=a[:,-1,:]-a[:,-2,:]\n",
    "    c=b.reshape(f,784)\n",
    "    return c\n",
    "\n",
    "def phigradx(model):\n",
    "    a=model.layers[0].weight.data\n",
    "    f=a.size(dim=0)\n",
    "    a=a.reshape(f,28,28)\n",
    "    b=torch.zeros(f,28,28)\n",
    "    b[:,:,0:-1]=a[:,:,1:]-a[:,:,0:-1]\n",
    "    b[:,:,-1]=a[:,:,-1]-a[:,:,-2]\n",
    "    c=b.reshape(f,784)\n",
    "    return c\n",
    "\n",
    "def H12norm(inp,model):\n",
    "    dfs=0.0\n",
    "    fs=0.0\n",
    "    pred=model(inp)\n",
    "    ts=pred.size(dim=0)\n",
    "    gpx=phigradx(model)\n",
    "    gpy=phigrady(model)\n",
    "    inp2=model.modelPhi(inp)\n",
    "    Psi=model.modelPsi(inp2)\n",
    "    dPsi=nth_derivative(Psi,wrt=inp2,n=1)\n",
    "    Dvecx=torch.mm(dPsi,gpx)\n",
    "    Dvecy=torch.mm(dPsi,gpy)\n",
    "    Dvecxsq=torch.square(Dvecx)\n",
    "    Dvecysq=torch.square(Dvecy)\n",
    "    Dvecsq=Dvecxsq+Dvecysq\n",
    "    dfsvec=torch.mul(Dvecsq,inp)\n",
    "    dfs=torch.sum(dfsvec)\n",
    "    dfs=1/ts*dfs\n",
    "    predt=torch.transpose(pred,0,1)\n",
    "    fs=1/ts*torch.mm(predt,pred)\n",
    "    res=torch.sqrt(dfs+fs)\n",
    "    return res.reshape(-1)\n",
    "\n",
    "def EulerLagrangeProduct(lmb,x_inp,y_inp,model_f,model_G):\n",
    "    S1=0\n",
    "    S2=0\n",
    "    S3=0\n",
    "    ts=x_inp.size(dim=0)\n",
    "    f_pred=model_f(x_inp)\n",
    "    y_inp=y_inp.reshape(ts,1)\n",
    "    fdiff=f_pred-y_inp\n",
    "    G_pred=model_G(x_inp)\n",
    "    G_pred_t=torch.transpose(G_pred,0,1)\n",
    "    S1=1/ts*torch.mm(G_pred_t,fdiff)\n",
    "    gpx_f=phigradx(model_f)\n",
    "    gpy_f=phigrady(model_f)\n",
    "    inp2_f=model_f.modelPhi(x_inp)\n",
    "    Psi_f=model_f.modelPsi(inp2_f)\n",
    "    dPsi_f=nth_derivative(Psi_f,wrt=inp2_f,n=1)\n",
    "    Dvecx_f=torch.mm(dPsi_f,gpx_f)\n",
    "    Dvecy_f=torch.mm(dPsi_f,gpy_f)\n",
    "    gpx_G=phigradx(model_G)\n",
    "    gpy_G=phigrady(model_G)\n",
    "    inp2_G=model_G.modelPhi(x_inp)\n",
    "    Psi_G=model_G.modelPsi(inp2_G)\n",
    "    dPsi_G=nth_derivative(Psi_G,wrt=inp2_G,n=1)\n",
    "    Dvecx_G=torch.mm(dPsi_G,gpx_G)\n",
    "    Dvecy_G=torch.mm(dPsi_G,gpy_G)\n",
    "    xProd=torch.mul(Dvecx_f,Dvecx_G)\n",
    "    yProd=torch.mul(Dvecy_f,Dvecy_G)\n",
    "    Prod=xProd+yProd\n",
    "    S3mat=torch.mul(Prod,x_inp)\n",
    "    S3=lmb/ts*torch.sum(S3mat)\n",
    "    val=S1+S3\n",
    "    S2S=H12norm(x_inp,model_G)\n",
    "    val=val/S2S\n",
    "    return val\n",
    "\n",
    "def LossFunction_f(lmb,x_inp,y_inp,model_f,model_G):\n",
    "    prod=EulerLagrangeProduct(lmb,x_inp,y_inp,model_f,model_G)\n",
    "    loss=torch.abs(prod)\n",
    "    return loss\n",
    "\n",
    "def LossFunction_G(lmb,x_inp,y_inp,model_f,model_G):\n",
    "    prod=EulerLagrangeProduct(lmb,x_inp,y_inp,model_f,model_G)\n",
    "    loss=-1.0*torch.abs(prod)\n",
    "    return loss\n",
    "\n",
    "def my_rel_loss(output,target):\n",
    "    loss=torch.abs(output-target)\n",
    "    loss=loss/target\n",
    "    loss=torch.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the choices $N_\\Xi =1$ and $N_\\Theta=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the neural network based on npot potentials\n",
    "nd=x_Train_t.size(dim=1)\n",
    "npot=2**10\n",
    "model_f=FFNN(nd,npot,non_trainable = False)\n",
    "model_G=FFNN(nd,npot,non_trainable = False)\n",
    "optimizer_f = torch.optim.Adam(model_f.parameters())\n",
    "optimizer_G = torch.optim.Adam(model_G.parameters())\n",
    "\n",
    "epochs_f=2\n",
    "epochs_G=1\n",
    "loops=200\n",
    "batch_size = 64\n",
    "lmb=0.001\n",
    "\n",
    "epoch_MRE=[]\n",
    "epoch_MRE_Test=[]\n",
    "epoch_list=[]\n",
    "\n",
    "for loopc in range(loops):\n",
    "    for epochs_G_c in range(epochs_G):\n",
    "        permutation = torch.randperm(x_Train_t.size()[0])\n",
    "        for i in range(0,x_Train_t.size()[0], batch_size):\n",
    "            optimizer_G.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "            loss=LossFunction_G(lmb,batch_x,batch_y,model_f,model_G)\n",
    "            loss.backward()\n",
    "            optimizer_G.step()\n",
    "    for epochs_f_c in range(epochs_f):\n",
    "        permutation = torch.randperm(x_Train_t.size()[0])\n",
    "        acc_MRE=0\n",
    "        acc_Loss=0\n",
    "        counter=0\n",
    "        for i in range(0,x_Train_t.size()[0], batch_size):\n",
    "            optimizer_f.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "            loss=LossFunction_f(lmb,batch_x,batch_y,model_f,model_G)\n",
    "            loss.backward()\n",
    "            optimizer_f.step()\n",
    "            y_pred=model_f(batch_x)\n",
    "            y_pred=y_pred.reshape(-1)\n",
    "            acc_MRE+=my_rel_loss(y_pred,batch_y)\n",
    "            acc_Loss+=loss\n",
    "            counter+=1\n",
    "    y_pred_test=model_f(x_Test_t)\n",
    "    y_pred_test=y_pred_test.reshape(-1)\n",
    "    MRE_Test=my_rel_loss(y_pred_test,y_Test_t)\n",
    "    print(MRE_Test)\n",
    "    epoch_MRE_Test.append(MRE_Test.detach().numpy())\n",
    "    MRE=acc_MRE/counter\n",
    "    print(MRE)\n",
    "    epoch_MRE.append(MRE.detach().numpy())\n",
    "    epoch_list.append(loopc+1)\n",
    "\n",
    "\n",
    "epoch_MRE=np.array(epoch_MRE)\n",
    "epoch_MRE_Test=np.array(epoch_MRE_Test)\n",
    "counter=np.array(epoch_list)\n",
    "\n",
    "\n",
    "#Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0.01, 0.5])\n",
    "plt.xlabel('number of epochs',fontsize=16)\n",
    "plt.ylabel('mean relative error',fontsize=16)\n",
    "plot_1a, =ax.semilogy(counter,epoch_MRE,color='blue',label='training set')\n",
    "plot_1b, =ax.semilogy(counter,epoch_MRE_Test,'--',color='green',label='test set')\n",
    "plt.axhline(y = 0.1594, color = 'r', linestyle = ':')\n",
    "ax.legend(handles=[plot_1a,plot_1b],loc='upper right',fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.grid()\n",
    "plt.grid(which='minor',alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set $N_\\Xi =2$ and $N_\\Theta=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the neural network based on np potentials\n",
    "nd=x_Train_t.size(dim=1)\n",
    "npot=2**10\n",
    "model_f=FFNN(nd,npot,non_trainable = False)\n",
    "model_G=FFNN(nd,npot,non_trainable = False)\n",
    "optimizer_f = torch.optim.Adam(model_f.parameters())\n",
    "optimizer_G = torch.optim.Adam(model_G.parameters())\n",
    "\n",
    "epochs_f=1\n",
    "epochs_G=2\n",
    "loops=200\n",
    "batch_size = 64\n",
    "lmb=0.001\n",
    "\n",
    "epoch_MRE=[]\n",
    "epoch_MRE_Test=[]\n",
    "epoch_list=[]\n",
    "\n",
    "for loopc in range(loops):\n",
    "    for epochs_G_c in range(epochs_G):\n",
    "        permutation = torch.randperm(x_Train_t.size()[0])\n",
    "        for i in range(0,x_Train_t.size()[0], batch_size):\n",
    "            optimizer_G.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "            loss=LossFunction_G(lmb,batch_x,batch_y,model_f,model_G)\n",
    "            loss.backward()\n",
    "            optimizer_G.step()\n",
    "    for epochs_f_c in range(epochs_f):\n",
    "        permutation = torch.randperm(x_Train_t.size()[0])\n",
    "        acc_MRE=0\n",
    "        acc_Loss=0\n",
    "        counter=0\n",
    "        for i in range(0,x_Train_t.size()[0], batch_size):\n",
    "            optimizer_f.zero_grad()\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "            loss=LossFunction_f(lmb,batch_x,batch_y,model_f,model_G)\n",
    "            loss.backward()\n",
    "            optimizer_f.step()\n",
    "            y_pred=model_f(batch_x)\n",
    "            y_pred=y_pred.reshape(-1)\n",
    "            acc_MRE+=my_rel_loss(y_pred,batch_y)\n",
    "            acc_Loss+=loss\n",
    "            counter+=1\n",
    "    y_pred_test=model_f(x_Test_t)\n",
    "    y_pred_test=y_pred_test.reshape(-1)\n",
    "    MRE_Test=my_rel_loss(y_pred_test,y_Test_t)\n",
    "    epoch_MRE_Test.append(MRE_Test.detach().numpy())\n",
    "    MRE=acc_MRE/counter\n",
    "    epoch_MRE.append(MRE.detach().numpy())\n",
    "    epoch_list.append(loopc+1)\n",
    "\n",
    "\n",
    "epoch_MRE=np.array(epoch_MRE)\n",
    "epoch_MRE_Test=np.array(epoch_MRE_Test)\n",
    "counter=np.array(epoch_list)\n",
    "\n",
    "\n",
    "#Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0.01, 0.5])\n",
    "plt.xlabel('number of epochs',fontsize=16)\n",
    "plt.ylabel('mean relative error',fontsize=16)\n",
    "plot_1a, =ax.semilogy(counter,epoch_MRE,color='blue',label='training set')\n",
    "plot_1b, =ax.semilogy(counter,epoch_MRE_Test,'--',color='green',label='test set')\n",
    "plt.axhline(y = 0.1594, color = 'r', linestyle = ':')\n",
    "ax.legend(handles=[plot_1a,plot_1b],loc='upper right',fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.grid()\n",
    "plt.grid(which='minor',alpha=0.2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
