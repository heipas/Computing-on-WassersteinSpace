{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook we run the experiments of Section 6.2 for the MNIST dataset.\n",
    "\n",
    "First of all, we have to import the required libraries as well as the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from CF_NeuralNetwork import FFNN\n",
    "from CF_NeuralNetwork_PC import FFNNPC\n",
    "\n",
    "#Load the training set and normalize it\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train=X_train/X_train.sum(axis=(1,2))[:,None,None]\n",
    "X_train=np.reshape(X_train,(np.size(X_train,0),28*28))\n",
    "X_test=X_test/X_test.sum(axis=(1,2))[:,None,None]\n",
    "X_test=np.reshape(X_test,(np.size(X_test,0),28*28))\n",
    "Y_train=np.loadtxt('MNIST_D_train.gz', delimiter=',')\n",
    "Y_test=np.loadtxt('MNIST_D_test.gz', delimiter=',')\n",
    "X_tot=np.concatenate((X_test,X_train),axis=0)\n",
    "Y_tot=np.concatenate((Y_train,Y_test),axis=0)\n",
    "\n",
    "# Transform data to torch variables\n",
    "x_Train_t=Variable(torch.from_numpy(X_train).float(), requires_grad=True)\n",
    "y_Train_t=Variable(torch.from_numpy(Y_train).float(), requires_grad=False)\n",
    "x_Test_t=Variable(torch.from_numpy(X_test).float(), requires_grad=False)\n",
    "y_Test_t=Variable(torch.from_numpy(Y_test).float(), requires_grad=False)\n",
    "x_Tot_t=Variable(torch.from_numpy(X_tot).float(), requires_grad=False)\n",
    "\n",
    "# Our loss function is the mean relative error\n",
    "def my_rel_loss(output,target):\n",
    "    loss=torch.abs(output-target)\n",
    "    loss=loss/target\n",
    "    loss=torch.mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 6.4: In this experiment, the part of the neural network representing the maxima function is fixed, whilst the first hidden layer is randomly initialized and trainable. We consider number_potentials=2^12 neurons in the first hidden layer and train the neural network over 300 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the neural network\n",
    "dimension_space=x_Train_t.size(dim=1)\n",
    "number_potentials=2**12\n",
    "model=FFNN(dimension_space,number_potentials,True)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs=300\n",
    "batch_size = 64\n",
    "\n",
    "# Training Loss\n",
    "epoch_Loss=[]\n",
    "# List for the epochs\n",
    "epoch_list=[]\n",
    "# Test Loss\n",
    "epoch_Loss_Val=[]\n",
    "\n",
    "for epochc in range(epochs):\n",
    "    permutation = torch.randperm(x_Train_t.size()[0])\n",
    "    acc_loss=0\n",
    "    counter=0\n",
    "    for i in range(0,x_Train_t.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "        y_pred=model(batch_x)\n",
    "        y_pred=y_pred.reshape(-1)\n",
    "        loss=my_rel_loss(y_pred,batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc_loss += loss\n",
    "        counter += 1\n",
    "    loss_app=acc_loss/counter\n",
    "    y_pred_val=model(x_Test_t)\n",
    "    y_pred_val=y_pred_val.reshape(-1)\n",
    "    loss_Val=my_rel_loss(y_pred_val,y_Test_t)\n",
    "    epoch_Loss_Val.append(loss_Val.detach().numpy())\n",
    "    epoch_Loss.append(loss_app.detach().numpy())\n",
    "    epoch_list.append(epochc+1)\n",
    "\n",
    "# Create numpy arrays for the plots\n",
    "counter=np.array(epoch_list)\n",
    "mse_loss=np.array(epoch_Loss)\n",
    "mse_loss_val=np.array(epoch_Loss_Val)\n",
    "\n",
    "# Generate the plots\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0.01, 0.5])\n",
    "plt.xlabel('number of epochs',fontsize=16)\n",
    "plt.ylabel('mean relative error',fontsize=16)\n",
    "plot_1a, =ax.semilogy(counter,mse_loss,color='blue',label='training set')\n",
    "plot_1b, =ax.semilogy(counter,mse_loss_val,'--',color='green',label='test set')\n",
    "plt.axhline(y = 0.159429, color = 'r', linestyle = ':')\n",
    "ax.legend(handles=[plot_1a,plot_1b],loc='upper right',fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.grid()\n",
    "plt.grid(which='minor',alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 6.5: We repeat the experiment from before, however the first hidden layer will be initialized with 2^12 pre-computed Kantorovich potentials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the neural network with the pre-computed Kantorovich Potentials\n",
    "\n",
    "bary0=np.loadtxt('BarycenterZero.gz', delimiter=',')\n",
    "B0f=np.loadtxt('MNIST_P_f.gz', delimiter=',')\n",
    "B0g=np.loadtxt('MNIST_P_g.gz', delimiter=',')\n",
    "bary0=np.reshape(bary0,28*28)\n",
    "bvectot=np.matmul(B0g,bary0)\n",
    "idx=np.arange(B0f.shape[0])\n",
    "np.random.shuffle(idx)\n",
    "BS=B0f[idx,:]\n",
    "bvecS=bvectot[idx]\n",
    "dimension_space=x_Train_t.size(dim=1)\n",
    "number_potentials=2**12\n",
    "Amat=BS[:number_potentials,:]\n",
    "bvec=bvecS[0:number_potentials]\n",
    "\n",
    "model=FFNNPC(Amat,bvec,dimension_space,number_potentials,True)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs=300\n",
    "batch_size = 64\n",
    "# Training Loss\n",
    "epoch_Loss=[]\n",
    "# List for the epochs\n",
    "epoch_list=[]\n",
    "# Test Loss\n",
    "epoch_Loss_Val=[]\n",
    "\n",
    "# Initial erros; i.e., based on the pre-computed potentials, but without any training\n",
    "y_pred_train=model(x_Train_t)\n",
    "y_pred_train=y_pred_train.reshape(-1)\n",
    "loss_train=my_rel_loss(y_pred_train,y_Train_t)\n",
    "y_pred_test=model(x_Test_t)\n",
    "y_pred_test=y_pred_test.reshape(-1)\n",
    "loss_test=my_rel_loss(y_pred_test,y_Test_t)\n",
    "epoch_Loss_Val.append(loss_test.detach().numpy())\n",
    "epoch_Loss.append(loss_train.detach().numpy())\n",
    "epoch_list.append(0)\n",
    "\n",
    "\n",
    "for epochc in range(epochs):\n",
    "    permutation = torch.randperm(x_Train_t.size()[0])\n",
    "    acc_loss=0\n",
    "    counter=0\n",
    "    for i in range(0,x_Train_t.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = x_Train_t[indices], y_Train_t[indices]\n",
    "        y_pred=model(batch_x)\n",
    "        y_pred=y_pred.reshape(-1)\n",
    "        loss=my_rel_loss(y_pred,batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc_loss += loss\n",
    "        counter += 1\n",
    "    loss_app=acc_loss/counter\n",
    "    y_pred_val=model(x_Test_t)\n",
    "    y_pred_val=y_pred_val.reshape(-1)\n",
    "    loss_Val=my_rel_loss(y_pred_val,y_Test_t)\n",
    "    epoch_Loss_Val.append(loss_Val.detach().numpy())\n",
    "    epoch_Loss.append(loss_app.detach().numpy())\n",
    "    epoch_list.append(epochc+1)\n",
    "\n",
    "# Create numpy arrays for the plots\n",
    "counter=np.array(epoch_list)\n",
    "mse_loss=np.array(epoch_Loss)\n",
    "mse_loss_val=np.array(epoch_Loss_Val)\n",
    "\n",
    "\n",
    "# Generate the plots\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0.01, 0.5])\n",
    "plt.xlabel('number of epochs',fontsize=16)\n",
    "plt.ylabel('mean relative error',fontsize=16)\n",
    "plot_1a, =ax.semilogy(counter,mse_loss,color='blue',label='training set')\n",
    "plot_1b, =ax.semilogy(counter,mse_loss_val,'--',color='green',label='test set')\n",
    "plt.axhline(y = 0.159429, color = 'r', linestyle = ':')\n",
    "ax.legend(handles=[plot_1a,plot_1b],loc='upper right',fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.grid()\n",
    "plt.grid(which='minor',alpha=0.2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
